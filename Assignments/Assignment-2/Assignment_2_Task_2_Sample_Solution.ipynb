{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Graph Embeddings using the Biomedical Wikidata knowledge graph\n",
        "\n",
        "One way to mine and complete missing information from KGs is to use Knowledge Graph Embedding (KGE) techniques. To apply KGE methods on the KG data, you can use one of the existing approaches, such as DGL-KE, that allow you to train your data with selection of KGE models and adjustable machine learning metrics on its framework. We will use DGL-KE (https://github.com/awslabs/dgl-ke) package for the Assignment 2 solution to apply KG embeddings to the biomedical subgraph from Wikidata KG to generate vector space representation of entities and relations of the KG."
      ],
      "metadata": {
        "id": "YS_2h1RdzMjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with DGL-KE package installation and its requirements to provide the environment in which the package can run."
      ],
      "metadata": {
        "id": "XyV7stXI1D-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo pip3 install dgl-cu101==0.4.3.post2\n",
        "!git clone  https://github.com/awslabs/dgl-ke.git\n",
        "!pushd dgl-ke;cd python;sudo python3 setup.py install;\n",
        "!pip3 uninstall torch -y\n",
        "!sudo pip3 install torch==1.5.0+cu101 torchvision==0.6.0+cu101\n",
        "!sudo pip3 install ogb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na4pFFq1Jdw4",
        "outputId": "576ad33d-7b7a-4f79-86f7-edfdb1b4d44a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Graph Embeddings with DGL-KE\n",
        " \n",
        "  Provided dataset already contains train and test sets so we can skip data preparation step for creation of train, test and valid sets. For DGL-KE, training set is only necessary to generate KG embeddings without evaluation step, or we can add optional test and valid sets to see the model performance with the ranking metrics (MRR and Hit@K) to have an idea about the quality of the resulting embedding model."
      ],
      "metadata": {
        "id": "Mdf3PPuV1z71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DGL-KE provides a set of commands to run training, evaluation, prediction steps for the KGE. We can change command line parameters with different options provided. For example, we can choose one of {TransE, TransE_l1, TransE_l2, TransR, RESCAL, DistMult, ComplEx, RotatE, SimplE} embedding model for our training using our preferred model with model_name parameter.\n",
        "You can see detailed explanations for the DGL-KE training command line parameters below:\n",
        "*   ***dglke_train*** trains KG embeddings on CPUs or GPUs in a single machine and saves the trained node embeddings and relation embeddings on disks.\n",
        "*   ***--model_name*** {TransE, TransE_l1, TransE_l2, TransR, RESCAL, DistMult, ComplEx, RotatE, SimplE} The models provided by DGL-KE.\n",
        "*   ***--data_path DATA_PATH*** The path of the directory where DGL-KE loads knowledge graph data.\n",
        "*   ***--dataset DATA_SET*** The name of the knowledge graph stored under data_path. If it is one of the builtin knowledge grpahs such as FB15k, FB15k-237, wn18, wn18rr, and Freebase, DGL-KE will automatically download the knowledge graph and keep it under data_path.\n",
        "*   ***--format FORMAT*** The format of the dataset. For builtin knowledge graphs, the format is determined automatically. For users own knowledge graphs, it needs to be raw_udd_{htr} or udd_{htr}. raw_udd_ indicates that the user's data use raw ID for entities and relations and udd_ indicates that the user's data uses KGE ID. {htr} indicates the location of the head entity, tail entity and relation in a triplet. For example, htr means the head entity is the first element in the triplet, the tail entity is the second element and the relation is the last element.\n",
        "*   ***--data_files [DATA_FILES ...]*** A list of data file names. This is required for training KGE on their own datasets. If the format is raw_udd_{htr}, users need to provide train_file [valid_file] [test_file]. If the format is udd_{htr}, users need to provide entity_file relation_file train_file [valid_file] [test_file]. In both cases, valid_file and test_file are optional.\n",
        "*   ***--neg_sample_size NEG_SAMPLE_SIZE*** The number of negative samples we use for each positive sample in the training.\n",
        "*   ***--hidden_dim HIDDEN_DIM*** The embedding size of relations and entities.\n",
        "*   ***-g GAMMA*** or ***--gamma GAMMA*** The margin value in the score function. It is used by TransX and RotatE.\n",
        "*   ***--max_step MAX_STEP*** The maximal number of steps to train the model in a single process. A step trains the model with a batch of data. In the case of multiprocessing training, the total number of training steps is MAX_STEP * NUM_PROC.\n",
        "*   ***--lr LR*** The learning rate. DGL-KE uses Adagrad to optimize the model parameters.\n",
        "*   ***--batch_size BATCH_SIZE*** The batch size for training.\n",
        "\n",
        "For more command explanations: https://github.com/awslabs/dgl-ke/tree/master/docs/source \n"
      ],
      "metadata": {
        "id": "ZFPaKtO83gmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!DGLBACKEND=pytorch dglke_train --model_name SimplE --dataset wikibio --batch_size 1000 --log_interval 100 \\\n",
        "--neg_sample_size 200 --regularization_coef=1e-9 --hidden_dim 400 --gamma 19.9 \\\n",
        "--lr 0.25 --batch_size_eval 16 --gpu 0 -adv --max_step 25000  --save_path /content \\\n",
        "--data_path /content --format raw_udd_hrt --data_files /content/biomedical_kg_train.txt --neg_sample_size_eval 10000 \n"
      ],
      "metadata": {
        "id": "_BPR9jasKQQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6add6a-e1ef-41e1-b2f3-2e9b94f35520"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KG Completion / Inference\n",
        " \n",
        "  DGL-KE package accepts candidate head (h), relation (r) and tail (t) elements as .list extension files to calculate the probability of missing triples in the KG. All combinations of these elements are considered as candidate triples and a score for each triple is calculated. Embedding models will assign different probability scores for candidate triples based on the model chosen. For example for Trans E, the learnt embedding of (h+r) should be close to the learnt embedding of t if the triple is positive or correct. This distance increases for negative or false triples. The distance between (h+r) and t is measured with L1 or L2 distance in Trans E. Generating the prediction scores using different embedding methods and selecting the best model based on evaluation dataset is recommended to obtain more robust predictions."
      ],
      "metadata": {
        "id": "zgJgsAXg5M-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the KG completion task, we have provided different test triples for each group. As mentioned above, we will generate head file with unique subjects of triples in test group file. As we descrribed in the Assignment 2 instructions, we'd like to obtain possible new disease-drug links with the specified relation \"drug or therapy used for treatment (P2176)\". Our relation file will contain P2176 relation. To score each drug we will generate tail file with all unique drugs in the KG training set. "
      ],
      "metadata": {
        "id": "46ijyuPM-n2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "head = []\n",
        "rel = []\n",
        "tail = []\n",
        "drug_rel = \"<http://www.wikidata.org/prop/direct/P2176>\"\n",
        "df =pd.read_csv(\"/content/test_group1.txt\", names=['subject','predicate','object',\"_\"],  delimiter=\"\\t\")\n",
        "df_all =pd.read_csv(\"/content/biomedical_kg_train.txt\", names=['subject','predicate','object',\"_\"],  delimiter=\"\\t\")\n",
        "#tail all drugs\n",
        "df_d = df_all[df_all['predicate'].str.contains(drug_rel)]\n",
        "df_t = df_d['object'].unique()\n",
        "#head\n",
        "df_h = df['subject'].unique()\n",
        "with open(\"/content/head.list\", \"w\") as fl:\n",
        "    for h in df_h:\n",
        "      fl.write(h + \"\\n\")\n",
        "#rel\n",
        "df_r = df['predicate'].unique()\n",
        "with open(\"/content/rel.list\", \"w\") as fl:\n",
        "    for r in df_r:\n",
        "      fl.write(r +\"\\n\")\n",
        "#tail\n",
        "with open(\"/content/tail.list\", \"w\") as fl:\n",
        "    for t in df_t:\n",
        "      fl.write(t+ \"\\n\")"
      ],
      "metadata": {
        "id": "ohJld3QYOt7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the KG completion task, DGL-KE uses dglke_predict command to evaluate head.list, rel.list and tail.list element combinations. According to their distance, it assigns a score for each possible triple combination. Beside head, rel and tail list files we should also provide the trained model, and entity and relation mapping files. Since we'd like to calculate the Hit@10 metric of the predictions we limit the command with top 10 results."
      ],
      "metadata": {
        "id": "AHLPyHXrAUyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!DGLBACKEND=pytorch dglke_predict --model_path /content/SimplE_wikibio_0 --format 'h_r_t' --gpu 0 --entity_mfile entities.tsv --rel_mfile relations.tsv --data_files head.list rel.list tail.list --raw_data --score_func logsigmoid --topK 10 --exec_mode 'batch_head'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45jtDXOpLTAR",
        "outputId": "c9f0d9bf-1429-4c5f-f694-b37048ff5da4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat result.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVq77mxgfoW9",
        "outputId": "809deb67-0778-42d0-d1c5-2bdd3d00e96d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can normalize scores in results between 0 and 1 range to calculate the confidence scores"
      ],
      "metadata": {
        "id": "gPNvRgOUbGYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "new_rows = []\n",
        "df_pred =pd.read_csv(\"/content/result.tsv\", names=['subject','predicate','object',\"score\"],  delimiter=\"\\t\")\n",
        "all_scores = df_pred[\"score\"][1:].astype(float)\n",
        "with open(\"/content/result.tsv\", \"r\") as fl:\n",
        "    next(fl)\n",
        "    for r in fl:\n",
        "      splitted = r[:-1].split(\"\\t\")\n",
        "      normalised = (float(splitted[3]) - np.min(all_scores)) / (np.max(all_scores) - np.min(all_scores))\n",
        "      newline = (\"{}\\t{}\\t{}\\t{}\\n\".format(splitted[0], splitted[1], splitted[2], normalised))\n",
        "      new_rows.append(newline)\n",
        "with open(\"/content/normalised_result.tsv\", \"w\") as fl:\n",
        "  for row in new_rows:\n",
        "    fl.write(row+ \"\\n\")\n",
        "\n",
        "!cat normalised_result.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HQV5CV0OD-z",
        "outputId": "b5863576-2404-4c26-9f87-a9e79e1e057a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hit@10 Score Table\n",
        "To calculate Hit@10 metric of produced top 10 predictions we'll check for each triple in our test group to see if there is a match between the test group triple and top 10 result of predictions. Match has found means Hit@10 score is 1, otherwise 0. We should repeat this for each triple in the test group and the average of the Hit@10 scores will give the final Hit@10 score.\n",
        "\n",
        "Test Group | TransE | ComplEx | SimplE\n",
        "--- | --- | --- | --- \n",
        "Test Group 1 | **0.5** |**0**|**0.5**\n",
        "Test Group 2 | **0.5** |**0.5** |**0**\n",
        "Test Group 3 | **0.33** |**0.33**|**0.33**\n",
        "Test Group 4 | **0** |**0**|**0**\n",
        "Test Group 5 | **0** |**0** |**0**\n",
        "Test Group 6 | **0** |**0**|**0**\n",
        "Test Group 7 | **0** |**0**|**0**\n",
        "Test Group 8 | **0** |**0**|**0**\n",
        "Test Group 9 | **0** |**0**|**0**\n",
        "Test Group 10 | **0** |**0**|**0**\n",
        "\n",
        "The models that we select generated different Hit@10 scores for test groups. Additionally, these scores can be improved with other embedding model selections which might be a better fit for the traning data.\n"
      ],
      "metadata": {
        "id": "GYjZKUUgZDKr"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Assignment 2- Task 2 Sample Solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}